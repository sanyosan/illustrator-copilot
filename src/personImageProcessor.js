const sharp = require('sharp');
const Tesseract = require('tesseract.js');
const faceapi = require('@vladmandic/face-api');
const canvas = require('canvas');
const fs = require('fs').promises;
const path = require('path');
const EventEmitter = require('events');

// Canvasç’°å¢ƒã®è¨­å®š
const { Canvas, Image, ImageData } = canvas;
faceapi.env.monkeyPatch({ Canvas, Image, ImageData });

class PersonImageProcessor extends EventEmitter {
    constructor(config = {}) {
        super();
        this.config = {
            inputDir: config.inputDir || './data/seated_photos',
            outputDir: config.outputDir || './data/processed',
            modelsDir: config.modelsDir || './models',
            targetWidth: config.targetWidth || 800,
            targetHeight: config.targetHeight || 1200,
            headTopMargin: config.headTopMargin || 50,
            ...config
        };
        
        this.isModelLoaded = false;
        this.processingQueue = [];
        this.results = new Map();
    }
    
    async init() {
        try {
            console.log('ğŸ§  AI models ã‚’åˆæœŸåŒ–ä¸­...');
            
            // Face-APIãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
            await this.loadFaceModels();
            
            // ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
            await this.ensureDirectories();
            
            console.log('âœ… PersonImageProcessor ãŒåˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸ');
            this.isModelLoaded = true;
            
            return true;
        } catch (error) {
            console.error('âŒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼:', error);
            return false;
        }
    }
    
    async loadFaceModels() {
        try {
            const modelPath = this.config.modelsDir;
            
            // å¿…è¦ãªãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆåˆå›ã®ã¿ï¼‰
            await this.downloadModelsIfNeeded();
            
            // Face-APIãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
            await faceapi.nets.tinyFaceDetector.loadFromDisk(modelPath);
            await faceapi.nets.faceLandmark68Net.loadFromDisk(modelPath);
            await faceapi.nets.faceRecognitionNet.loadFromDisk(modelPath);
            
            console.log('âœ… Face-API models loaded');
        } catch (error) {
            console.error('âŒ Face model loading error:', error);
            // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ¢ãƒ‡ãƒ«ãªã—ã§å‹•ä½œ
            console.log('âš ï¸ Face detection will be disabled');
        }
    }
    
    async downloadModelsIfNeeded() {
        const modelFiles = [
            'tiny_face_detector_model-weights_manifest.json',
            'tiny_face_detector_model-shard1',
            'face_landmark_68_model-weights_manifest.json',
            'face_landmark_68_model-shard1',
            'face_recognition_model-weights_manifest.json',
            'face_recognition_model-shard1'
        ];
        
        const modelPath = this.config.modelsDir;
        
        // modelsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
        try {
            await fs.access(modelPath);
        } catch {
            await fs.mkdir(modelPath, { recursive: true });
        }
        
        // ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        const missingFiles = [];
        for (const file of modelFiles) {
            try {
                await fs.access(path.join(modelPath, file));
            } catch {
                missingFiles.push(file);
            }
        }
        
        if (missingFiles.length > 0) {
            console.log('ğŸ“¥ Face-APIãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...');
            console.log('âš ï¸ å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ‰‹å‹•ã§modelsãƒ•ã‚©ãƒ«ãƒ€ã«é…ç½®ã—ã¦ãã ã•ã„');
            // å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ã“ã“ã§ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å‡¦ç†ã‚’è¡Œã„ã¾ã™
        }
    }
    
    async ensureDirectories() {
        const dirs = [
            this.config.inputDir,
            this.config.outputDir,
            this.config.modelsDir
        ];
        
        for (const dir of dirs) {
            try {
                await fs.access(dir);
            } catch {
                await fs.mkdir(dir, { recursive: true });
            }
        }
    }
    
    async processAllImages() {
        console.log('ğŸš€ ç”»åƒå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...');
        
        if (!this.isModelLoaded) {
            console.log('â³ ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã‚’å¾…æ©Ÿä¸­...');
            await this.init();
        }
        
        try {
            const files = await fs.readdir(this.config.inputDir);
            const imageFiles = files.filter(file => 
                /\.(jpg|jpeg|png|bmp)$/i.test(file)
            );
            
            console.log(`ğŸ“¸ ${imageFiles.length} æšã®ç”»åƒã‚’ç™ºè¦‹ã—ã¾ã—ãŸ`);
            
            let processed = 0;
            const results = [];
            
            for (const file of imageFiles) {
                try {
                    console.log(`\nğŸ”„ å‡¦ç†ä¸­: ${file}`);
                    const result = await this.processImage(file);
                    results.push(result);
                    processed++;
                    
                    this.emit('imageProcessed', {
                        file,
                        result,
                        progress: processed / imageFiles.length
                    });
                    
                } catch (error) {
                    console.error(`âŒ ${file} ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼:`, error.message);
                    results.push({
                        filename: file,
                        success: false,
                        error: error.message
                    });
                }
            }
            
            console.log(`\nğŸ‰ å‡¦ç†å®Œäº†: ${processed}/${imageFiles.length} æš`);
            return results;
            
        } catch (error) {
            console.error('âŒ ç”»åƒå‡¦ç†ã‚¨ãƒ©ãƒ¼:', error);
            throw error;
        }
    }
    
    async processImage(filename) {
        const inputPath = path.join(this.config.inputDir, filename);
        const baseName = path.parse(filename).name;
        const outputPath = path.join(this.config.outputDir, `${baseName}_processed.jpg`);
        
        console.log(`  ğŸ“¥ ç”»åƒã‚’èª­ã¿è¾¼ã¿ä¸­: ${filename}`);
        
        // Sharp ã§ç”»åƒã‚’èª­ã¿è¾¼ã¿
        const image = sharp(inputPath);
        const metadata = await image.metadata();
        
        console.log(`  ğŸ“ å…ƒç”»åƒã‚µã‚¤ã‚º: ${metadata.width}x${metadata.height}`);
        
        // ç”»åƒã‚’Bufferã¨ã—ã¦å–å¾—ï¼ˆFace-APIç”¨ï¼‰
        const imageBuffer = await image.png().toBuffer();
        
        let faceBox = null;
        let landmarks = null;
        
        // é¡”æ¤œå‡ºã®å®Ÿè¡Œ
        try {
            console.log('  ğŸ” é¡”æ¤œå‡ºã‚’å®Ÿè¡Œä¸­...');
            const detectionResult = await this.detectFace(imageBuffer);
            if (detectionResult) {
                faceBox = detectionResult.box;
                landmarks = detectionResult.landmarks;
                console.log(`  âœ… é¡”ã‚’æ¤œå‡º: x=${faceBox.x}, y=${faceBox.y}, w=${faceBox.width}, h=${faceBox.height}`);
            } else {
                console.log('  âš ï¸ é¡”ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ');
            }
        } catch (error) {
            console.log('  âš ï¸ é¡”æ¤œå‡ºã‚’ã‚¹ã‚­ãƒƒãƒ—:', error.message);
        }
        
        // OCRï¼ˆæ–‡å­—èªè­˜ï¼‰ã®å®Ÿè¡Œ
        let ocrResult = null;
        try {
            console.log('  ğŸ“ æ–‡å­—èªè­˜ã‚’å®Ÿè¡Œä¸­...');
            ocrResult = await this.performOCR(imageBuffer);
            console.log(`  âœ… èªè­˜ã•ã‚ŒãŸæ–‡å­—: ${ocrResult.numbers || 'ãªã—'}, ${ocrResult.name || 'ãªã—'}`);
        } catch (error) {
            console.log('  âš ï¸ æ–‡å­—èªè­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—:', error.message);
        }
        
        // ç”»åƒã®åˆ‡ã‚Šå–ã‚Šã¨åŠ å·¥
        console.log('  âœ‚ï¸ ç”»åƒã‚’åŠ å·¥ä¸­...');
        const processedImage = await this.cropAndResize(image, metadata, faceBox);
        
        // çµæœã‚’ä¿å­˜
        await processedImage.jpeg({ quality: 90 }).toFile(outputPath);
        
        const result = {
            filename,
            outputPath,
            success: true,
            originalSize: { width: metadata.width, height: metadata.height },
            processedSize: { width: this.config.targetWidth, height: this.config.targetHeight },
            faceDetected: !!faceBox,
            faceBox,
            landmarks,
            ocr: ocrResult,
            timestamp: new Date()
        };
        
        // çµæœã‚’JSONã¨ã—ã¦ä¿å­˜
        const resultPath = path.join(this.config.outputDir, `${baseName}_result.json`);
        await fs.writeFile(resultPath, JSON.stringify(result, null, 2));
        
        console.log(`  âœ… å‡¦ç†å®Œäº†: ${outputPath}`);
        return result;
    }
    
    async detectFace(imageBuffer) {
        try {
            // Canvasè¦ç´ ã‚’ä½œæˆ
            const img = new Image();
            
            return new Promise((resolve, reject) => {
                img.onload = async () => {
                    try {
                        const detections = await faceapi
                            .detectAllFaces(img, new faceapi.TinyFaceDetectorOptions())
                            .withFaceLandmarks();
                        
                        if (detections && detections.length > 0) {
                            // æœ€å¤§ã®é¡”ã‚’é¸æŠ
                            const detection = detections.reduce((prev, current) => 
                                (prev.detection.box.width * prev.detection.box.height) > 
                                (current.detection.box.width * current.detection.box.height) ? prev : current
                            );
                            
                            resolve({
                                box: detection.detection.box,
                                landmarks: detection.landmarks
                            });
                        } else {
                            resolve(null);
                        }
                    } catch (error) {
                        reject(error);
                    }
                };
                
                img.onerror = reject;
                img.src = imageBuffer;
            });
        } catch (error) {
            console.error('Face detection error:', error);
            return null;
        }
    }
    
    async performOCR(imageBuffer) {
        try {
            const worker = await Tesseract.createWorker('jpn+eng');
            
            const { data: { text } } = await worker.recognize(imageBuffer);
            await worker.terminate();
            
            // 4æ¡ã®æ•°å­—ã‚’æŠ½å‡º
            const numberMatch = text.match(/\d{4}/);
            const numbers = numberMatch ? numberMatch[0] : null;
            
            // æ—¥æœ¬èªã®æ–‡å­—ï¼ˆæ°åï¼‰ã‚’æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
            const japaneseMatch = text.match(/[ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠæ¼¢å­—]+/);
            const name = japaneseMatch ? japaneseMatch[0] : null;
            
            return {
                rawText: text.trim(),
                numbers,
                name,
                confidence: 0.8 // å®Ÿéš›ã¯OCRã®ä¿¡é ¼åº¦ã‚’ä½¿ç”¨
            };
        } catch (error) {
            console.error('OCR error:', error);
            return null;
        }
    }
    
    async cropAndResize(image, metadata, faceBox) {
        let cropParams = {
            left: 0,
            top: 0,
            width: metadata.width,
            height: metadata.height
        };
        
        if (faceBox) {
            // é¡”ã‚’ä¸­å¿ƒã¨ã—ãŸåˆ‡ã‚Šå–ã‚Šç¯„å›²ã‚’è¨ˆç®—
            const faceCenterX = faceBox.x + faceBox.width / 2;
            const faceCenterY = faceBox.y + faceBox.height / 2;
            
            // é ­ã®ã¦ã£ãºã‚“ã‚ˆã‚Š50pxä¸Šã‹ã‚‰é–‹å§‹
            const topY = Math.max(0, faceBox.y - this.config.headTopMargin);
            
            // ç¸¦é•·ã®æ¯”ç‡ã‚’ç¶­æŒã—ã¦åˆ‡ã‚Šå–ã‚Šç¯„å›²ã‚’è¨ˆç®—
            const aspectRatio = this.config.targetWidth / this.config.targetHeight;
            let cropWidth = metadata.height * aspectRatio;
            let cropHeight = metadata.height;
            
            // å¹…ãŒç”»åƒã‚’ã¯ã¿å‡ºã™å ´åˆã¯èª¿æ•´
            if (cropWidth > metadata.width) {
                cropWidth = metadata.width;
                cropHeight = metadata.width / aspectRatio;
            }
            
            // é¡”ã‚’ä¸­å¿ƒã«ã—ã¦åˆ‡ã‚Šå–ã‚Šä½ç½®ã‚’èª¿æ•´
            let cropLeft = faceCenterX - cropWidth / 2;
            cropLeft = Math.max(0, Math.min(cropLeft, metadata.width - cropWidth));
            
            // ä¸Šç«¯ã‚’èª¿æ•´ï¼ˆé ­ã‚ˆã‚Š50pxä¸Šã‹ã‚‰ï¼‰
            let cropTop = topY;
            cropTop = Math.max(0, Math.min(cropTop, metadata.height - cropHeight));
            
            cropParams = {
                left: Math.round(cropLeft),
                top: Math.round(cropTop),
                width: Math.round(cropWidth),
                height: Math.round(cropHeight)
            };
            
            console.log(`  ğŸ“ åˆ‡ã‚Šå–ã‚Šç¯„å›²: x=${cropParams.left}, y=${cropParams.top}, w=${cropParams.width}, h=${cropParams.height}`);
        }
        
        // ç”»åƒã‚’åˆ‡ã‚Šå–ã£ã¦ã€æŒ‡å®šã‚µã‚¤ã‚ºã«ãƒªã‚µã‚¤ã‚º
        return image
            .extract(cropParams)
            .resize(this.config.targetWidth, this.config.targetHeight, {
                fit: 'cover',
                position: 'center'
            });
    }
    
    async getProcessingStatus() {
        return {
            isModelLoaded: this.isModelLoaded,
            queueLength: this.processingQueue.length,
            resultsCount: this.results.size,
            config: this.config
        };
    }
}

module.exports = PersonImageProcessor;
